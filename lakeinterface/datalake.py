# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_s3.ipynb.

# %% auto 0
__all__ = ['S3ObjectNotFound', 'Datalake']

# %% ../nbs/02_s3.ipynb 2
import boto3
import polars as pl
import s3fs
import json
from dateutil.parser import parse

from io import BytesIO

from lakeinterface.config import ConfigManager


# %% ../nbs/02_s3.ipynb 3
def most_recent(keys, prefix):
    dates = [
        o.replace(prefix, '').replace('data.parquet', '').replace('/', '')
        for o in keys
    ]
    latest = max(parse(d) for d in dates).strftime('%Y%m%d')
    return f'{prefix}/{latest}/data.parquet'


# %% ../nbs/02_s3.ipynb 4
class S3ObjectNotFound(Exception):
    pass


class Datalake(object):
    """
    A class to wrap interface to an AWS S3 datalake
    Implemented as a singleton to reduce number of live sessions
    ...

    Attributes
    ----------
    session: a boto3 session
    s3 : a boto3 S3 client
    bucket : S3 bucket location of lake
    
    Methods
    -------
    __init__(config, profile='default'):
        Initializes the AWS S3 client using AWS profile_name and dict of parameters from ConfigManager
    
    get_object(key):
        Core method for loading objects using boto3 S3 client
    
    load_csv(key, delimiter=',', skiprows=None, line_terminator=None):
        Loads csv object with S3 prefix = key
    
    load_json(key):
        Loads json object with S3 prefix = key
        
    list_objects(prefix):
        Lists all objects with S3 prefix = key
    
    save_json(path, data, timestamp=None):
        Saves json object to specified path with an optional timestamp that will be inserted into path
    
    put_object(key, data, metadata={}):
        Core method for saving objects using boto3 S3 client
    
    most_recent(prefix):
        For a given S3 prefix returns object has most recent timestamp
     
    put(path, df, timestamp=None):
        Saves a dataframe as parquet to specified path with an optional timestamp that will be inserted into path
    
    get(path):
        Loads parquet object from specified path as a dataframe

    """
    _instance = None

    def __new__(cls, config, profile_name=None):
        if cls._instance is None:
            cls._instance = super(Datalake, cls).__new__(cls)
            # Put any initialization here.
        return cls._instance
    
    def __init__(self, config, profile_name=None):
        self.session = boto3.session.Session(profile_name=profile_name)
        
        self.bucket = config.get('bucket')
        self.s3 = self.session.client('s3')
        self.fs = s3fs.S3FileSystem(profile=profile_name)
        
    def get_object(self, key):
        try:
            return self.s3.get_object(Bucket=self.bucket, Key=key)
        except Exception as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                raise S3ObjectNotFound('No S3 object with key = %s' % key)
            else:
                raise

    def load_csv(self,key, separator=',', skiprows=None, line_terminator=None):
        obj = self.get_object(key)
        if line_terminator:
            return pl.read_csv(obj['Body'], separator=separator, lineterminator=line_terminator)
        else:
            return pl.read_csv(obj['Body'], separator=separator)
    
    
    def load_json(self, key):
        obj = self.get_object(key)
        return json.loads(obj['Body'].read())
    
    def load_parquet(self, key):
        obj = self.get_object(key)
        return pl.read_parquet(BytesIO(obj['Body'].read()))
    
    def get(self, path):
        try:
            key = self.most_recent(path)
        except Exception as e:
            print(f'No objects found with path: {key}. {e}')
            return None

        return self.load_parquet(key)
    
    
    def list_objects(self, prefix):
        
        paginator = self.s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=self.bucket, Prefix=prefix)

        return sum([[obj['Key'] for obj in page['Contents']] for page in pages], [])
    

    def save_json(self, path, data, timestamp=None):
        if timestamp:
            key = f'{path}/timestamp={timestamp}/data.json'
        else:
            key = f'{path}/data.json'

        return self.put_object(key, json.dumps(data))
        
    def put_object(self, key, data, metadata={}):
        try:
            resp = self.s3.put_object(
                Bucket=self.bucket,
                Key=key,
                Body=data
            )
            status_code = resp['ResponseMetadata']['HTTPStatusCode']
            if status_code == 200:
                return True
            else:
                raise Exception(f'Unknown error. Status code: {status_code}')
        except Exception as e:
            raise Exception(f'Unknown error in put object for {key}. {str(e)}')

    
            
    def put(self, path, df, timestamp=None):
        if timestamp:
            key = f'{path}/{timestamp}/data.parquet'
        else:
            key = f'{path}/data.parquet'

        with self.fs.open(f'{self.bucket}/{key}', mode='wb') as f:
            df.write_parquet(f)
        
        return
    
    def most_recent(self, prefix):
        matched_objects = self.list_objects(prefix=prefix)
        
        if len(matched_objects) > 1:
            try:
                return most_recent(matched_objects, prefix)
            except:
                print(f'Multiple objects found for prefix {prefix}. Unable to find most recent.')
                return None
        elif len(matched_objects) == 0:
            print(f'No objects found for prefix {prefix}')
            return None
        else:
            return matched_objects[0]

    

