{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067deb1-a0bd-4822-939c-2c1256e90d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datalake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664de47-b532-4afd-b632-10f88e8db8f6",
   "metadata": {},
   "source": [
    "## Datalake interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390df4bf-d926-44f7-ad19-d076fabdfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "#| hide\n",
    "\n",
    "import boto3\n",
    "import polars as pl\n",
    "import s3fs\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "from lakeinterface.config import ConfigManager\n",
    "from lakeinterface.config import lake_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904d701-920c-486a-b676-60671970e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "def most_recent(keys, prefix):\n",
    "    file_types = {o.split('/')[-1] for o in keys}\n",
    "    \n",
    "    if len(file_types) > 1:\n",
    "        raise Exception(f\"Mixed filetypes found with prefix {prefix}: {','.join(file_types)}\")\n",
    "    else:\n",
    "        file_type = next(iter(file_types))\n",
    "\n",
    "    dates = [\n",
    "        o.replace(prefix, '').replace(file_type, '').replace('/', '')\n",
    "        for o in keys\n",
    "    ]\n",
    "\n",
    "    timestamp_lengths = {len(d) for d in dates}\n",
    "    \n",
    "    if len({len(d) for d in dates}) > 1:\n",
    "        raise Exception(f'Mixed timestamp formats found with prefix {prefix}')\n",
    "    else:\n",
    "        latest = max(dates)\n",
    "    \n",
    "    return f'{prefix}/{latest}/{file_type}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ed9c9-1993-47f4-a49d-db708a2138a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e4020-260f-4007-9889-b4628beb7d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7b8ac-ee8f-40d6-a8a6-cd74cfd4d4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91459878-07c3-4219-afd2-62e2b60c0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "DEFAULT_REGION = 'us-east-1'\n",
    "\n",
    "class S3ObjectNotFound(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Datalake(object):\n",
    "    \"\"\"\n",
    "    A class to wrap interface to an AWS S3 datalake\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    session: a boto3 session\n",
    "    s3 : a boto3 S3 client\n",
    "    bucket : S3 bucket location of lake\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __init__(config_name, aws_profile='default'):\n",
    "        Initializes the AWS S3 client using AWS profile_name and dict of parameters from ConfigManager\n",
    "    \n",
    "    get_object(key):\n",
    "        Core method for loading objects using boto3 S3 client\n",
    "    \n",
    "    load_csv(key, delimiter=',', skiprows=None, line_terminator=None):\n",
    "        Loads csv object with S3 prefix = key\n",
    "    \n",
    "    load_json(key):\n",
    "        Loads json object with S3 prefix = key\n",
    "        \n",
    "    list_objects(prefix):\n",
    "        Lists all objects with S3 prefix = key\n",
    "    \n",
    "    save_json(path, data, timestamp=None):\n",
    "        Saves json object to specified path with an optional timestamp that will be inserted into path\n",
    "    \n",
    "    put_object(key, data, metadata={}):\n",
    "        Core method for saving objects using boto3 S3 client\n",
    "    \n",
    "    most_recent(prefix):\n",
    "        For a given S3 prefix returns object has most recent timestamp\n",
    "     \n",
    "    put(path, df, timestamp=None):\n",
    "        Saves a dataframe as parquet to specified path with an optional timestamp that will be inserted into path\n",
    "        \n",
    "    upload(file_obj, destination_folder, filename, timestamp=None):\n",
    "        Function for saving general file formats to specified destination in S3 with an optional timestamp that will be inserted into path\n",
    "    \n",
    "    get(path):\n",
    "        Loads parquet object from specified path as a dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_name, aws_profile=None):\n",
    "        if aws_profile:\n",
    "            self.session = boto3.session.Session(profile_name=aws_profile)\n",
    "        else:\n",
    "            self.session = boto3.session.Session(region_name=DEFAULT_REGION)\n",
    "        \n",
    "        config = lake_config(config_name, aws_profile=aws_profile)\n",
    "        self.bucket = config.get('bucket')\n",
    "        self.s3 = self.session.client('s3')\n",
    "        self.fs = s3fs.S3FileSystem(profile=aws_profile)\n",
    "        \n",
    "    def get_object(self, key):\n",
    "        try:\n",
    "            return self.s3.get_object(Bucket=self.bucket, Key=key)\n",
    "        except Exception as e:\n",
    "            if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "                raise S3ObjectNotFound('No S3 object with key = %s' % key)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def load_csv(self,key, separator=',', skiprows=None, line_terminator=None):\n",
    "        obj = self.get_object(key)\n",
    "        if line_terminator:\n",
    "            return pl.read_csv(obj['Body'], separator=separator, lineterminator=line_terminator)\n",
    "        else:\n",
    "            return pl.read_csv(obj['Body'], separator=separator)\n",
    "    \n",
    "    \n",
    "    def load_json(self, key):\n",
    "        obj = self.get_object(key)\n",
    "        return json.loads(obj['Body'].read())\n",
    "    \n",
    "    def load_parquet(self, key):\n",
    "        obj = self.get_object(key)\n",
    "        return pl.read_parquet(BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    def get(self, path):\n",
    "        try:\n",
    "            key = self.most_recent(path)\n",
    "        except Exception as e:\n",
    "            print(f'No objects found with path: {key}. {e}')\n",
    "            return None\n",
    "\n",
    "        file_type = key.split('/')[-1]\n",
    "        if file_type not in ['data.parquet', 'data.json']:\n",
    "            raise Exception(f'.get not implemented for files of type {file_type}')\n",
    "        \n",
    "        if file_type == 'data.parquet':\n",
    "            return self.load_parquet(key)\n",
    "        elif file_type == 'data.json':\n",
    "            return self.load_json(key)\n",
    "    \n",
    "    \n",
    "    def list_objects(self, prefix):\n",
    "        \n",
    "        paginator = self.s3.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=self.bucket, Prefix=prefix)\n",
    "\n",
    "        return sum([[obj['Key'] for obj in page['Contents']] for page in pages], [])\n",
    "    \n",
    "\n",
    "    def save_json(self, path, data, timestamp=None):\n",
    "        if timestamp:\n",
    "            key = f'{path}/{timestamp}/data.json'\n",
    "        else:\n",
    "            key = f'{path}/data.json'\n",
    "\n",
    "        return self.put_object(key, json.dumps(data))\n",
    "        \n",
    "    def put_object(self, key, data, metadata={}):\n",
    "        try:\n",
    "            resp = self.s3.put_object(\n",
    "                Bucket=self.bucket,\n",
    "                Key=key,\n",
    "                Body=data\n",
    "            )\n",
    "            status_code = resp['ResponseMetadata']['HTTPStatusCode']\n",
    "            if status_code == 200:\n",
    "                return True\n",
    "            else:\n",
    "                raise Exception(f'Unknown error. Status code: {status_code}')\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Unknown error in put object for {key}. {str(e)}')\n",
    "\n",
    "    \n",
    "            \n",
    "    def put(self, path, df, timestamp=None):\n",
    "        if timestamp:\n",
    "            key = f'{path}/{timestamp}/data.parquet'\n",
    "        else:\n",
    "            key = f'{path}/data.parquet'\n",
    "\n",
    "        with self.fs.open(f'{self.bucket}/{key}', mode='wb') as f:\n",
    "            df.write_parquet(f)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def upload(self, file_obj, destination_folder, filename, timestamp=None):\n",
    "        if timestamp:\n",
    "            key = f'{destination_folder}/{timestamp}/{filename}'\n",
    "        else:\n",
    "            key = f'{destination_folder}/{filename}'\n",
    "            \n",
    "        return self.s3.upload_fileobj(\n",
    "            Fileobj=file_obj,\n",
    "            Bucket=self.bucket,\n",
    "            Key=key\n",
    "        )\n",
    "    \n",
    "    def most_recent(self, prefix):\n",
    "        matched_objects = self.list_objects(prefix=prefix)\n",
    "        \n",
    "        if len(matched_objects) > 1:\n",
    "            try:\n",
    "                return most_recent(matched_objects, prefix)\n",
    "            except:\n",
    "                print(f'Multiple objects found for prefix {prefix}. Unable to find most recent.')\n",
    "                return None\n",
    "        elif len(matched_objects) == 0:\n",
    "            print(f'No objects found for prefix {prefix}')\n",
    "            return None\n",
    "        else:\n",
    "            return matched_objects[0]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c866ced-621c-4cdd-8308-a403bd4f9455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf95a1-7291-400f-a8e9-23db446351dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A class to wrap interface to an AWS S3 datalake\n",
      "    ...\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    session: a boto3 session\n",
      "    s3 : a boto3 S3 client\n",
      "    bucket : S3 bucket location of lake\n",
      "    \n",
      "    Methods\n",
      "    -------\n",
      "    __init__(config_name, aws_profile='default'):\n",
      "        Initializes the AWS S3 client using AWS profile_name and dict of parameters from ConfigManager\n",
      "    \n",
      "    get_object(key):\n",
      "        Core method for loading objects using boto3 S3 client\n",
      "    \n",
      "    load_csv(key, delimiter=',', skiprows=None, line_terminator=None):\n",
      "        Loads csv object with S3 prefix = key\n",
      "    \n",
      "    load_json(key):\n",
      "        Loads json object with S3 prefix = key\n",
      "        \n",
      "    list_objects(prefix):\n",
      "        Lists all objects with S3 prefix = key\n",
      "    \n",
      "    save_json(path, data, timestamp=None):\n",
      "        Saves json object to specified path with an optional timestamp that will be inserted into path\n",
      "    \n",
      "    put_object(key, data, metadata={}):\n",
      "        Core method for saving objects using boto3 S3 client\n",
      "    \n",
      "    most_recent(prefix):\n",
      "        For a given S3 prefix returns object has most recent timestamp\n",
      "     \n",
      "    put(path, df, timestamp=None):\n",
      "        Saves a dataframe as parquet to specified path with an optional timestamp that will be inserted into path\n",
      "        \n",
      "    upload(file_obj, destination_folder, filename, timestamp=None):\n",
      "        Function for saving general file formats to specified destination in S3 with an optional timestamp that will be inserted into path\n",
      "    \n",
      "    get(path):\n",
      "        Loads parquet object from specified path as a dataframe\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Datalake.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf620ae6-da95-4792-91a4-87e410ac2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake = Datalake('bankdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa49aa3-902d-48bf-b555-106d0a2269b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71094d8a-215c-40ca-a2a0-fe80f8cef9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c88e7-d622-4c8d-99ba-f76c26783d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>col1</th><th>col2</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>3</td></tr><tr><td>2</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────┬──────┐\n",
       "│ col1 ┆ col2 │\n",
       "│ ---  ┆ ---  │\n",
       "│ i64  ┆ i64  │\n",
       "╞══════╪══════╡\n",
       "│ 1    ┆ 3    │\n",
       "│ 2    ┆ 4    │\n",
       "└──────┴──────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pl.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3989bf-c493-482c-864e-6b1f22292caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake.put('test/example1', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfcb56-22a9-4f29-94f6-0aba765b5fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/example1/data.parquet',\n",
       " 'test/example2/data.parquet',\n",
       " 'test/example3/data.parquet',\n",
       " 'test/general_upload/20230926/nbdev.yml',\n",
       " 'test/h8/H8_H8.xsd',\n",
       " 'test/h8/H8_data.xml',\n",
       " 'test/h8/H8_struct.xml',\n",
       " 'test/h8/frb_common.xsd',\n",
       " 'test/xml_put/4114567.xml']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lake.list_objects(prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cac35-1a60-4ec0-8133-2f75fe65ad3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>col1</th><th>col2</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>3</td></tr><tr><td>2</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────┬──────┐\n",
       "│ col1 ┆ col2 │\n",
       "│ ---  ┆ ---  │\n",
       "│ i64  ┆ i64  │\n",
       "╞══════╪══════╡\n",
       "│ 1    ┆ 3    │\n",
       "│ 2    ┆ 4    │\n",
       "└──────┴──────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lake.get('test/example1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404bd84-6db7-438a-b662-de1f42fba8ce",
   "metadata": {},
   "source": [
    "Uploading general file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380013f-c027-4906-8791-be9a84a30644",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nbdev.yml', 'rb') as f:\n",
    "    lake.upload(f, 'test/general_upload', 'nbdev.yml', timestamp='20230926')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48794297-7bc6-43fd-aafa-e34c161cb0fd",
   "metadata": {},
   "source": [
    "### Testing polars\n",
    "\n",
    "There are multiple ways of having polars load & save data from S3. This was to test out approaches for which approach to take in the Datalake class defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83d1e4-8229-4d2b-987c-1b012e377152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec139470-78e7-4391-bbc3-0b0480ce5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "bucket = lake.bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54d114-1b30-487e-a198-8e7b761675dd",
   "metadata": {},
   "source": [
    "#### Reading parquet file I\n",
    "\n",
    "Doesn't work with scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8b2a8-27ce-49a0-a930-28a564755b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45589590072631836\n"
     ]
    }
   ],
   "source": [
    "path = 'test/example1/data.parquet'\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "dataset = pq.ParquetDataset(f\"s3://{bucket}/{path}\", filesystem=fs)\n",
    "df = pl.from_arrow(dataset.read())\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49429bce-05be-44e1-a2ac-ba4374eead94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>col1</th><th>col2</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>3</td></tr><tr><td>2</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────┬──────┐\n",
       "│ col1 ┆ col2 │\n",
       "│ ---  ┆ ---  │\n",
       "│ i64  ┆ i64  │\n",
       "╞══════╪══════╡\n",
       "│ 1    ┆ 3    │\n",
       "│ 2    ┆ 4    │\n",
       "└──────┴──────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d47e4b-748a-42fb-baed-2f38bacb84cc",
   "metadata": {},
   "source": [
    "#### Reading parquet file II\n",
    "\n",
    "using pyarrow dataset to specify format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fd4a7-df93-4462-a24d-c836d49cd2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "┌──────┬──────┐\n",
      "│ col1 ┆ col2 │\n",
      "│ ---  ┆ ---  │\n",
      "│ i64  ┆ i64  │\n",
      "╞══════╪══════╡\n",
      "│ 1    ┆ 3    │\n",
      "│ 2    ┆ 4    │\n",
      "└──────┴──────┘\n",
      "0.3325178623199463\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dataset2 = ds.dataset(f\"s3://{bucket}/{path}\", filesystem=fs, format='parquet')\n",
    "df_parquet = pl.scan_pyarrow_dataset(dataset2)\n",
    "\n",
    "print(df_parquet.collect().head())\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc06f3-f44a-447a-9606-cc26aadac231",
   "metadata": {},
   "source": [
    "#### Reading parquet file III\n",
    "\n",
    "using boto3 get_object\n",
    "\n",
    "Doesn't allow scanning but approach works for csv and json files too. Appears to be quicker too\n",
    "\n",
    "This is first choice and an easy switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e702c12-8670-447e-b629-6f74e4334277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17659497261047363\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "resp = lake.get_object('test/example2/data.parquet')\n",
    "df = pl.read_parquet(BytesIO(resp['Body'].read()))\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c36b1f-4048-4554-85b5-ddc7f3766b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4bd483b-45e7-4eed-9678-74e479e7634f",
   "metadata": {},
   "source": [
    "#### Writing parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cbb42-afbc-4295-876f-837ed7ffe65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>col1</th><th>col2</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>3</td></tr><tr><td>2</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌──────┬──────┐\n",
       "│ col1 ┆ col2 │\n",
       "│ ---  ┆ ---  │\n",
       "│ i64  ┆ i64  │\n",
       "╞══════╪══════╡\n",
       "│ 1    ┆ 3    │\n",
       "│ 2    ┆ 4    │\n",
       "└──────┴──────┘"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pl.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b62e75-2d3f-4c28-913d-9ecc49912105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09860610961914062\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "with fs.open(f'{bucket}/test/example3/data.parquet', mode='wb') as f:\n",
    "    df.write_parquet(f)\n",
    "    \n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188629c4-e128-44c0-9a1f-c2ef477aa781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "┌──────┬──────┐\n",
      "│ col1 ┆ col2 │\n",
      "│ ---  ┆ ---  │\n",
      "│ i64  ┆ i64  │\n",
      "╞══════╪══════╡\n",
      "│ 1    ┆ 3    │\n",
      "│ 2    ┆ 4    │\n",
      "└──────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "path = 'test/example3/data.parquet'\n",
    "dataset2 = ds.dataset(f\"s3://{bucket}/{path}\", filesystem=fs, format='parquet')\n",
    "df_parquet = pl.scan_pyarrow_dataset(dataset2)\n",
    "print(df_parquet.collect().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1b5ce-379d-41db-a18f-9765a0f6876e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e73c0f-e24a-4fcf-bbff-c2220b1c7a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
